import gzip
import json
import os
import time
import requests
from datetime import datetime

# é…ç½®
REPO_OWNER = "helloamger"
REPO_NAME = "JiNan-Company-Rating"
CATEGORY_ID = "DIC_kwDORKDfUs4C19oY"
OUTPUT_FILE = "discussions.json"
CHECKPOINT_FILE = "discussions_checkpoint.json"
GITHUB_API_URL = "https://api.github.com/graphql"
OUTPUT_GZIP_FILE = "discussions.json.gz"

# è·å– GitHub Token
def get_github_token():
    token = os.environ.get('GITHUB_TOKEN')
    return token

# GraphQL æŸ¥è¯¢
def get_discussions_query(cursor=None):
    # å¦‚æœæœ‰ cursorï¼Œæ·»åŠ  after å‚æ•°
    after_clause = f', after: "{cursor}"' if cursor else ""

    query = f'''
    query {{
      repository(owner: "{REPO_OWNER}", name: "{REPO_NAME}") {{
        discussions(
          first: 100
          categoryId: "{CATEGORY_ID}"
          orderBy: {{field: CREATED_AT, direction: ASC}}
          {after_clause}
        ) {{
          pageInfo {{
            hasNextPage
            endCursor
          }}
          edges {{
            node {{
              number
              bodyHTML
              title
              createdAt
              url
            }}
          }}
        }}
      }}
    }}
    '''
    return query


# å‘é€ GraphQL è¯·æ±‚ï¼Œå¸¦é‡è¯•æœºåˆ¶
def execute_graphql_with_retry(query, token, max_retries=3, retry_delay=5):
    headers = {
        'Authorization': f'Bearer {token}',
        'Content-Type': 'application/json',
    }

    for attempt in range(max_retries):
        try:
            response = requests.post(
                GITHUB_API_URL,
                json={'query': query},
                headers=headers,
                timeout=30
            )

            # æ£€æŸ¥ HTTP é”™è¯¯
            response.raise_for_status()

            data = response.json()

            # æ£€æŸ¥ GraphQL é”™è¯¯
            if 'errors' in data:
                error_msg = data['errors'][0].get('message', 'Unknown GraphQL error')
                print(f"GraphQL é”™è¯¯: {error_msg}")

                # å¦‚æœæ˜¯ rate limit é”™è¯¯ï¼Œç­‰å¾…æ›´é•¿æ—¶é—´
                if 'rate limit' in error_msg.lower():
                    reset_time = int(response.headers.get('X-RateLimit-Reset', 0))
                    if reset_time:
                        wait_time = max(reset_time - int(time.time()), 0) + 5
                        print(f"è¾¾åˆ°é€Ÿç‡é™åˆ¶ï¼Œç­‰å¾… {wait_time} ç§’...")
                        time.sleep(wait_time)
                        continue

                if attempt < max_retries - 1:
                    time.sleep(retry_delay)
                    continue
                else:
                    raise Exception(f"GraphQL é”™è¯¯: {error_msg}")

            return data

        except requests.exceptions.RequestException as e:
            print(f"è¯·æ±‚å¤±è´¥ (å°è¯• {attempt + 1}/{max_retries}): {e}")
            if attempt < max_retries - 1:
                wait_time = retry_delay * (attempt + 1)  # æŒ‡æ•°é€€é¿
                print(f"ç­‰å¾… {wait_time} ç§’åé‡è¯•...")
                time.sleep(wait_time)
            else:
                raise Exception(f"è¯·æ±‚å¤±è´¥ï¼Œå·²é‡è¯• {max_retries} æ¬¡: {e}")

    return None


# åŠ è½½æ£€æŸ¥ç‚¹
def load_checkpoint():
    if os.path.exists(CHECKPOINT_FILE):
        try:
            with open(CHECKPOINT_FILE, 'r', encoding='utf-8') as f:
                return json.load(f)
        except Exception as e:
            print(f"åŠ è½½æ£€æŸ¥ç‚¹å¤±è´¥: {e}")
    return {
        'discussions': [],
        'last_cursor': None,
        'has_more': True,
        'total_count': 0
    }


# ä¿å­˜æ£€æŸ¥ç‚¹
def save_checkpoint(checkpoint_data):
    try:
        with open(CHECKPOINT_FILE, 'w', encoding='utf-8') as f:
            json.dump(checkpoint_data, f, ensure_ascii=False, indent=2)
    except Exception as e:
        print(f"ä¿å­˜æ£€æŸ¥ç‚¹å¤±è´¥: {e}")


# ä¿å­˜æœ€ç»ˆç»“æœ
def save_final_result(discussions):
    output_data = {
        'repository': f"{REPO_OWNER}/{REPO_NAME}",
        'category_id': CATEGORY_ID,
        'total_count': len(discussions),
        'fetched_at': datetime.now().isoformat(),
        'discussions': discussions
    }

    with open(OUTPUT_FILE, 'w', encoding='utf-8') as f:
        json.dump(output_data, f, ensure_ascii=False, indent=2)

    print(f"\nâœ… æˆåŠŸä¿å­˜ {len(discussions)} æ¡ discussions åˆ° {OUTPUT_FILE}")

    # 2. ä¿å­˜ GZIP å‹ç¼©ç‰ˆæœ¬ï¼ˆç”Ÿäº§ç”¨ï¼‰
    json_str = json.dumps(output_data, ensure_ascii=False)
    json_bytes = json_str.encode('utf-8')

    with gzip.open(OUTPUT_GZIP_FILE, 'wb', compresslevel=9) as f:
        f.write(json_bytes)

    compressed_size = os.path.getsize(OUTPUT_GZIP_FILE)
    compression_ratio = (1 - compressed_size / len(json_bytes)) * 100
    print(f"âœ… å·²ä¿å­˜ GZIP: {OUTPUT_GZIP_FILE} ({compressed_size} bytes, å‹ç¼©ç‡ {compression_ratio:.1f}%)")


# ä¸»å‡½æ•°
def fetch_discussions():
    token = get_github_token()

    # åŠ è½½ä¹‹å‰çš„è¿›åº¦
    checkpoint = load_checkpoint()
    all_discussions = checkpoint['discussions']
    cursor = checkpoint['last_cursor']
    has_more = checkpoint['has_more']

    print(f"å¼€å§‹è·å– discussions...")
    print(f"ä»“åº“: {REPO_OWNER}/{REPO_NAME}")
    print(f"åˆ†ç±» ID: {CATEGORY_ID}")

    if all_discussions:
        print(f"ä»æ£€æŸ¥ç‚¹æ¢å¤ï¼Œå·²æœ‰ {len(all_discussions)} æ¡è®°å½•ï¼Œç»§ç»­è·å–...")

    page_count = 0

    try:
        while has_more:
            page_count += 1
            print(f"\nğŸ“„ è·å–ç¬¬ {page_count} é¡µ...")

            query = get_discussions_query(cursor)
            data = execute_graphql_with_retry(query, token)

            if not data or 'data' not in data:
                print("âš ï¸ æœªè·å–åˆ°æ•°æ®ï¼Œä¿å­˜å½“å‰è¿›åº¦å¹¶é€€å‡º...")
                break

            discussions_data = data['data']['repository']['discussions']
            edges = discussions_data['edges']
            page_info = discussions_data['pageInfo']

            # å¤„ç†å½“å‰é¡µçš„æ•°æ®
            new_discussions = []
            for edge in edges:
                node = edge['node']
                discussion = {
                    'number': node['number'],
                    'title': node['title'],
                    'created_at': node['createdAt'],
                    'url': node['url'],
                    'bodyHTML': node['bodyHTML']
                }
                new_discussions.append(discussion)

            all_discussions.extend(new_discussions)

            # æ›´æ–°çŠ¶æ€
            has_more = page_info['hasNextPage']
            cursor = page_info['endCursor'] if has_more else None

            print(f"æœ¬é¡µè·å– {len(new_discussions)} æ¡ï¼Œæ€»è®¡ {len(all_discussions)} æ¡")

            # ä¿å­˜æ£€æŸ¥ç‚¹
            checkpoint = {
                'discussions': all_discussions,
                'last_cursor': cursor,
                'has_more': has_more,
                'total_count': len(all_discussions)
            }
            save_checkpoint(checkpoint)

            # å¦‚æœè¿˜æœ‰æ›´å¤šï¼Œç­‰å¾…ä¸€å°æ®µæ—¶é—´é¿å…è§¦å‘é€Ÿç‡é™åˆ¶
            if has_more:
                time.sleep(25)

        # ä¿å­˜æœ€ç»ˆç»“æœ
        save_final_result(all_discussions)

        # æ¸…ç†æ£€æŸ¥ç‚¹æ–‡ä»¶ï¼ˆå¯é€‰ï¼‰
        if os.path.exists(CHECKPOINT_FILE):
            os.remove(CHECKPOINT_FILE)
            print(f"å·²æ¸…ç†æ£€æŸ¥ç‚¹æ–‡ä»¶ {CHECKPOINT_FILE}")

        return all_discussions

    except KeyboardInterrupt:
        print("\n\nâš ï¸ ç”¨æˆ·ä¸­æ–­ï¼Œä¿å­˜å½“å‰è¿›åº¦...")
        checkpoint = {
            'discussions': all_discussions,
            'last_cursor': cursor,
            'has_more': has_more if 'has_more' in locals() else True,
            'total_count': len(all_discussions)
        }
        save_checkpoint(checkpoint)
        print(f"è¿›åº¦å·²ä¿å­˜åˆ° {CHECKPOINT_FILE}ï¼Œä¸‹æ¬¡è¿è¡Œä¼šè‡ªåŠ¨æ¢å¤")
        return all_discussions

    except Exception as e:
        print(f"\nâŒ å‘ç”Ÿé”™è¯¯: {e}")
        print("ä¿å­˜å½“å‰è¿›åº¦...")
        checkpoint = {
            'discussions': all_discussions,
            'last_cursor': cursor if 'cursor' in locals() else None,
            'has_more': has_more if 'has_more' in locals() else True,
            'total_count': len(all_discussions)
        }
        save_checkpoint(checkpoint)
        raise


if __name__ == "__main__":
    try:
        discussions = fetch_discussions()
        print(f"\nğŸ‰ å®Œæˆï¼å…±è·å– {len(discussions)} æ¡ discussions")

        # æ˜¾ç¤ºå‰ 5 æ¡ä½œä¸ºé¢„è§ˆ
        if discussions:
            print("\né¢„è§ˆå‰ 5 æ¡:")
            for i, d in enumerate(discussions[:5], 1):
                print(f"  {i}. #{d['number']}: {d['title'][:50]}{'...' if len(d['title']) > 50 else ''}")

    except Exception as e:
        print(f"\nğŸ’¥ ç¨‹åºå¼‚å¸¸é€€å‡º: {e}")
        exit(1)
